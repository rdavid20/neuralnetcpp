
# ğŸ§  Neural Network From Scratch in C++

This is a minimal, self-contained neural network implementation written entirely in modern C++17. It includes a matrix math engine, training logic, multiple activation functions, and support for loading the Iris dataset â€” all without any external ML libraries.

## ğŸ“ Project Structure

```
â”œâ”€â”€ activation.hpp       # Activation functions and derivatives
â”œâ”€â”€ initializer.hpp      # Weight initialization strategies
â”œâ”€â”€ matrix.hpp           # Templated Matrix class with math operations
â”œâ”€â”€ neuralnetwork.hpp    # Core NeuralNet<T> class
â”œâ”€â”€ loader.{hpp,cpp}     # Dataset loading utilities (e.g. Iris, XOR)
â”œâ”€â”€ main.cpp             # Training + evaluation entry point
â”œâ”€â”€ Makefile             # Build instructions
â”œâ”€â”€ build/               # (Ignored) Compiled objects and binary
â”œâ”€â”€ datasets/            # (Ignored) Folder for Iris dataset and others
```

## ğŸš€ Building

Make sure you're using a compiler that supports **C++17** (e.g., g++ â‰¥ 7 or clang â‰¥ 6).

```bash
make
```

This will compile everything and place the executable in `build/neuralnet`.


## ğŸ§  Running the Iris Example

1. **Download the Iris dataset** from UCI:
   - https://archive.ics.uci.edu/ml/machine-learning-databases/iris/

2. **Place the file** in the `datasets/` folder:
   ```bash
   mkdir -p datasets
   mv iris.data datasets/iris.data
   ```

   The dataset should be CSV-formatted with no header, like:
   ```
   5.1,3.5,1.4,0.2,Iris-setosa
   4.9,3.0,1.4,0.2,Iris-setosa
   ...
   ```

3. **Run the program**:
   ```bash
   ./build/neuralnet
   ```


## ğŸ”§ Configuration

In `main.cpp`, you can configure:

```cpp
net.setActivation("Sigmoid");       // or "ReLU", "Tanh", etc.
net.pickInitializer("Xavier");      // or "He", "Uniform"
net.setLayerSizes({4, 6, 3});       // input, hidden, output layers
net.build();
```


## âœ… Features

- Clean Matrix<T> math engine
- Forward/backward propagation
- Modular `activation` and `initializer` interfaces
- Support for `sigmoid`, `tanh`, `ReLU`, `leaky ReLU`
- He and Xavier initialization schemes
- Training and evaluation on Iris dataset
- Clean separation of logic (main, model, data loader)


## ğŸ“Š Expected Accuracy

With `Sigmoid` + `Xavier` and `{4, 6, 3}` layers, you should see:
- **95â€“98% accuracy** on the Iris dataset after 1000 epochs
- Slight variation per run (due to random weight init)


## ğŸ‘¤ Author

Rasmus Davidsen
[@github.com/rdavid20](https://github.com/rdavid20)
